{"cells":[{"attachments":{},"cell_type":"markdown","id":"725fc7c7-c9e8-4d22-9379-d5a983fbef2e","metadata":{"language":"python"},"source":"## Agentic RAG System with LangChain & SingleStore Database\n\n### This tutorial will guide you through creating an intelligent agentic system that:\n\n#### Overview\n- Fetches Wikipedia content\n- Stores it as vectors in SingleStore\n- Uses LangChain agents to provide contextually rich responses\n- Implements retrieval-augmented generation (RAG)\n\n#### Prerequisites\n- Python 3.8+\n- SingleStore account - Create a database.\n- OpenAI API key"},{"attachments":{},"cell_type":"markdown","id":"4a989552-deb7-46b7-b86a-811ea910127a","metadata":{"language":"python"},"source":"### Step 1: Install Required Packages"},{"cell_type":"code","execution_count":9,"id":"99be1543-2e43-4ba4-af57-009ca7b896d9","metadata":{"execution":{"iopub.execute_input":"2025-06-05T15:52:18.014626Z","iopub.status.busy":"2025-06-05T15:52:18.014041Z","iopub.status.idle":"2025-06-05T15:52:55.104276Z","shell.execute_reply":"2025-06-05T15:52:55.100415Z","shell.execute_reply.started":"2025-06-05T15:52:18.014592Z"},"language":"python","trusted":true},"outputs":[],"source":"!pip install langchain-core langchain-openai langchain-community langchain --quiet\n!pip install singlestoredb --quiet\n!pip install wikipedia-api --quiet\n!pip install tiktoken --quiet\n!pip install numpy pandas --quiet"},{"attachments":{},"cell_type":"markdown","id":"7137a065-a147-4a84-afa9-b664dd11d328","metadata":{"language":"python"},"source":"### Step 2: Set Up Credentials and Imports"},{"cell_type":"code","execution_count":10,"id":"d3604fee-5779-4f8b-9873-0b8d37d63537","metadata":{"execution":{"iopub.execute_input":"2025-06-05T15:53:53.530332Z","iopub.status.busy":"2025-06-05T15:53:53.529908Z","iopub.status.idle":"2025-06-05T15:53:53.545440Z","shell.execute_reply":"2025-06-05T15:53:53.544607Z","shell.execute_reply.started":"2025-06-05T15:53:53.530297Z"},"language":"python","trusted":true},"outputs":[],"source":"import os\nimport warnings\nwarnings.filterwarnings('ignore')"},{"cell_type":"code","execution_count":12,"id":"b24e4b65-b6c7-46c2-91ed-4708a289ddf9","metadata":{"execution":{"iopub.execute_input":"2025-06-05T15:55:37.932875Z","iopub.status.busy":"2025-06-05T15:55:37.932187Z","iopub.status.idle":"2025-06-05T15:55:37.940197Z","shell.execute_reply":"2025-06-05T15:55:37.939433Z","shell.execute_reply.started":"2025-06-05T15:55:37.932852Z"},"language":"python","trusted":true},"outputs":[],"source":"os.environ['OPENAI_API_KEY'] = 'Add Your OpenAI API Key'\nSINGLESTORE_HOST = 'Add Your Singlestore Host'\nSINGLESTORE_PORT = 3306\nSINGLESTORE_USER = 'admin'\nSINGLESTORE_PASSWORD = 'Add Your Password'\nSINGLESTORE_DATABASE = 'Leema'"},{"cell_type":"code","execution_count":13,"id":"4bfc6de7-3672-420a-9e4f-e65fed33dba5","metadata":{"execution":{"iopub.execute_input":"2025-06-05T15:55:55.381715Z","iopub.status.busy":"2025-06-05T15:55:55.381172Z","iopub.status.idle":"2025-06-05T15:55:58.201208Z","shell.execute_reply":"2025-06-05T15:55:58.199719Z","shell.execute_reply.started":"2025-06-05T15:55:55.381676Z"},"language":"python","trusted":true},"outputs":[],"source":"import singlestoredb as s2\nfrom langchain_openai import OpenAIEmbeddings, ChatOpenAI\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.schema import Document\nfrom langchain.agents import initialize_agent, Tool, AgentType\nfrom langchain.memory import ConversationBufferMemory\nimport wikipediaapi\nimport numpy as np\nfrom typing import List"},{"attachments":{},"cell_type":"markdown","id":"3f4d73b1-7036-4405-9706-6adf63abd073","metadata":{"language":"python"},"source":"### Step 3: Set Up SingleStore Connection"},{"cell_type":"code","execution_count":14,"id":"6dca88c6-8714-4533-b57a-33b06f505282","metadata":{"execution":{"iopub.execute_input":"2025-06-05T15:56:20.672320Z","iopub.status.busy":"2025-06-05T15:56:20.671988Z","iopub.status.idle":"2025-06-05T15:56:22.068940Z","shell.execute_reply":"2025-06-05T15:56:22.067051Z","shell.execute_reply.started":"2025-06-05T15:56:20.672293Z"},"language":"python","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"‚úÖ Successfully connected to SingleStore!\n"}],"source":"def create_singlestore_connection():\n    try:\n        connection = s2.connect(\n            host=SINGLESTORE_HOST,\n            port=SINGLESTORE_PORT,\n            user=SINGLESTORE_USER,\n            password=SINGLESTORE_PASSWORD,\n            database=SINGLESTORE_DATABASE\n        )\n        print(\"‚úÖ Successfully connected to SingleStore!\")\n        return connection\n    except Exception as e:\n        print(f\"‚ùå Error connecting to SingleStore: {e}\")\n        return None\n\n# Test connection\nconn = create_singlestore_connection()"},{"attachments":{},"cell_type":"markdown","id":"771c8be8-1ce5-4937-9760-720536905260","metadata":{"language":"python"},"source":"### Step 4: Create Vector Table in SingleStore"},{"cell_type":"code","execution_count":15,"id":"ef9c97f5-15da-4ef9-a6e0-fa3eac2bf19d","metadata":{"execution":{"iopub.execute_input":"2025-06-05T15:56:49.109979Z","iopub.status.busy":"2025-06-05T15:56:49.109369Z","iopub.status.idle":"2025-06-05T15:56:49.360925Z","shell.execute_reply":"2025-06-05T15:56:49.359350Z","shell.execute_reply.started":"2025-06-05T15:56:49.109952Z"},"language":"python","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"‚úÖ Vector table created successfully!\n"}],"source":"def create_vector_table(connection):\n    \"\"\"Create a table for storing vectors and metadata\"\"\"\n    \n    create_table_query = \"\"\"\n    CREATE TABLE IF NOT EXISTS wikipedia_vectors (\n        id INT AUTO_INCREMENT PRIMARY KEY,\n        content TEXT,\n        title VARCHAR(500),\n        url VARCHAR(1000),\n        chunk_index INT,\n        embedding BLOB,\n        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n        INDEX(title),\n        INDEX(chunk_index)\n    );\n    \"\"\"\n    \n    try:\n        with connection.cursor() as cursor:\n            cursor.execute(create_table_query)\n            connection.commit()\n        print(\"‚úÖ Vector table created successfully!\")\n    except Exception as e:\n        print(f\"‚ùå Error creating table: {e}\")\n\n# Create the table\nif conn:\n    create_vector_table(conn)"},{"attachments":{},"cell_type":"markdown","id":"4deb3082-e749-4772-a136-af7d01a58041","metadata":{"language":"python"},"source":"### Step 5: Wikipedia Content Fetcher"},{"cell_type":"code","execution_count":16,"id":"324fe4f1-b05c-4fb7-b1ef-2453382e8953","metadata":{"execution":{"iopub.execute_input":"2025-06-05T15:57:15.345601Z","iopub.status.busy":"2025-06-05T15:57:15.345274Z","iopub.status.idle":"2025-06-05T15:57:15.351959Z","shell.execute_reply":"2025-06-05T15:57:15.351288Z","shell.execute_reply.started":"2025-06-05T15:57:15.345573Z"},"language":"python","trusted":true},"outputs":[],"source":"class WikipediaFetcher:\n    def __init__(self):\n        self.wiki = wikipediaapi.Wikipedia(\n            language='en',\n            extract_format=wikipediaapi.ExtractFormat.WIKI,\n            user_agent='LangChain-Tutorial/1.0'\n        )\n    \n    def fetch_page_content(self, page_title: str) -> dict:\n        \"\"\"Fetch content from a Wikipedia page\"\"\"\n        try:\n            page = self.wiki.page(page_title)\n            \n            if not page.exists():\n                print(f\"‚ùå Page '{page_title}' does not exist\")\n                return None\n            \n            return {\n                'title': page.title,\n                'content': page.text,\n                'url': page.fullurl,\n                'summary': page.summary[:500] + \"...\" if len(page.summary) > 500 else page.summary\n            }\n        except Exception as e:\n            print(f\"‚ùå Error fetching page '{page_title}': {e}\")\n            return None\n    \n    def fetch_multiple_pages(self, page_titles: List[str]) -> List[dict]:\n        \"\"\"Fetch multiple Wikipedia pages\"\"\"\n        pages = []\n        for title in page_titles:\n            print(f\"üìñ Fetching: {title}\")\n            page_data = self.fetch_page_content(title)\n            if page_data:\n                pages.append(page_data)\n        return pages"},{"cell_type":"code","execution_count":19,"id":"d0e04ab3-c042-42e8-a45a-9b2b8c174897","metadata":{"execution":{"iopub.execute_input":"2025-06-05T15:59:41.217837Z","iopub.status.busy":"2025-06-05T15:59:41.217294Z","iopub.status.idle":"2025-06-05T15:59:41.225491Z","shell.execute_reply":"2025-06-05T15:59:41.224202Z","shell.execute_reply.started":"2025-06-05T15:59:41.217813Z"},"language":"python","trusted":true},"outputs":[],"source":"wiki_fetcher = WikipediaFetcher()\n\n# Define some interesting topics to fetch\ntopics = [\n    \"Retrieval-augmented generation\",\n    \"Machine Learning\",\n    \"Natural Language Processing\",\n    \"Deep Learning\",\n    \"Computer Vision\",\n    \"database\"\n]"},{"cell_type":"code","execution_count":20,"id":"e112a77a-7654-4bfc-b27f-cfd2457769cc","metadata":{"execution":{"iopub.execute_input":"2025-06-05T15:59:46.579982Z","iopub.status.busy":"2025-06-05T15:59:46.579590Z","iopub.status.idle":"2025-06-05T15:59:47.880335Z","shell.execute_reply":"2025-06-05T15:59:47.878763Z","shell.execute_reply.started":"2025-06-05T15:59:46.579955Z"},"language":"python","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"üîÑ Fetching Wikipedia content...\nüìñ Fetching: Retrieval-augmented generation\nüìñ Fetching: Machine Learning\nüìñ Fetching: Natural Language Processing\nüìñ Fetching: Deep Learning\nüìñ Fetching: Computer Vision\nüìñ Fetching: database\n‚úÖ Fetched 6 pages successfully!\n"}],"source":"print(\"üîÑ Fetching Wikipedia content...\")\nwikipedia_pages = wiki_fetcher.fetch_multiple_pages(topics)\nprint(f\"‚úÖ Fetched {len(wikipedia_pages)} pages successfully!\")"},{"attachments":{},"cell_type":"markdown","id":"fbd797ab-64d4-470f-b11b-8a6f00ba3eaa","metadata":{"language":"python"},"source":"### Step 6: Text Chunking and Embedding"},{"cell_type":"code","execution_count":21,"id":"b9d276df-62a9-4ff7-9adc-d47e4ce46627","metadata":{"execution":{"iopub.execute_input":"2025-06-05T16:00:26.449728Z","iopub.status.busy":"2025-06-05T16:00:26.448789Z","iopub.status.idle":"2025-06-05T16:00:30.763968Z","shell.execute_reply":"2025-06-05T16:00:30.763367Z","shell.execute_reply.started":"2025-06-05T16:00:26.449604Z"},"language":"python","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"üîÑ Processing and chunking documents...\n‚úÖ Created 406 document chunks\nüîÑ Creating embeddings...\n‚úÖ Created 406 embeddings\n"}],"source":"class DocumentProcessor:\n    def __init__(self):\n        self.text_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=1000,\n            chunk_overlap=200,\n            length_function=len,\n        )\n        self.embeddings = OpenAIEmbeddings()\n    \n    def process_documents(self, pages: List[dict]) -> List[Document]:\n        \"\"\"Convert Wikipedia pages to LangChain documents and chunk them\"\"\"\n        documents = []\n        \n        for page in pages:\n            # Create LangChain Document\n            doc = Document(\n                page_content=page['content'],\n                metadata={\n                    'title': page['title'],\n                    'url': page['url'],\n                    'summary': page.get('summary', '')\n                }\n            )\n            \n            # Split into chunks\n            chunks = self.text_splitter.split_documents([doc])\n            \n            # Add chunk index to metadata\n            for i, chunk in enumerate(chunks):\n                chunk.metadata['chunk_index'] = i\n                documents.append(chunk)\n        \n        return documents\n    \n    def create_embeddings(self, documents: List[Document]) -> List[List[float]]:\n        \"\"\"Create embeddings for all document chunks\"\"\"\n        texts = [doc.page_content for doc in documents]\n        embeddings = self.embeddings.embed_documents(texts)\n        return embeddings\n\n# Process documents\nprocessor = DocumentProcessor()\nprint(\"üîÑ Processing and chunking documents...\")\ndocuments = processor.process_documents(wikipedia_pages)\nprint(f\"‚úÖ Created {len(documents)} document chunks\")\n\nprint(\"üîÑ Creating embeddings...\")\nembeddings = processor.create_embeddings(documents)\nprint(f\"‚úÖ Created {len(embeddings)} embeddings\")"},{"attachments":{},"cell_type":"markdown","id":"66d42dbb-39ba-4f25-8f24-44a410fdafcc","metadata":{"language":"python"},"source":"### Step 7: Store Vectors in SingleStore"},{"cell_type":"code","execution_count":22,"id":"96634026-ae32-4cb7-a952-c94a1b1bd191","metadata":{"execution":{"iopub.execute_input":"2025-06-05T16:01:03.928666Z","iopub.status.busy":"2025-06-05T16:01:03.928019Z","iopub.status.idle":"2025-06-05T16:01:36.089989Z","shell.execute_reply":"2025-06-05T16:01:36.088752Z","shell.execute_reply.started":"2025-06-05T16:01:03.928609Z"},"language":"python","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"‚úÖ Successfully stored 406 vectors in SingleStore!\n"}],"source":"def store_vectors_in_singlestore(connection, documents: List[Document], embeddings: List[List[float]]):\n    \"\"\"Store document chunks and their embeddings in SingleStore\"\"\"\n    \n    insert_query = \"\"\"\n    INSERT INTO wikipedia_vectors \n    (content, title, url, chunk_index, embedding) \n    VALUES (%s, %s, %s, %s, %s)\n    \"\"\"\n    \n    try:\n        with connection.cursor() as cursor:\n            for doc, embedding in zip(documents, embeddings):\n                # Convert embedding to bytes for storage\n                embedding_bytes = np.array(embedding, dtype=np.float32).tobytes()\n                \n                cursor.execute(insert_query, (\n                    doc.page_content,\n                    doc.metadata['title'],\n                    doc.metadata['url'],\n                    doc.metadata['chunk_index'],\n                    embedding_bytes\n                ))\n            \n            connection.commit()\n        \n        print(f\"‚úÖ Successfully stored {len(documents)} vectors in SingleStore!\")\n        \n    except Exception as e:\n        print(f\"‚ùå Error storing vectors: {e}\")\n\n# Store vectors\nif conn:\n    store_vectors_in_singlestore(conn, documents, embeddings)"},{"attachments":{},"cell_type":"markdown","id":"a8fb0f1c-f489-4d13-bbb1-ffaa4427db3c","metadata":{"language":"python"},"source":"### Step 8: Create Vector Retriever"},{"cell_type":"code","execution_count":23,"id":"187468da-b9e1-47a9-9b2c-d733433df379","metadata":{"execution":{"iopub.execute_input":"2025-06-05T16:02:54.575465Z","iopub.status.busy":"2025-06-05T16:02:54.575051Z","iopub.status.idle":"2025-06-05T16:02:55.395058Z","shell.execute_reply":"2025-06-05T16:02:55.394174Z","shell.execute_reply.started":"2025-06-05T16:02:54.575429Z"},"language":"python","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"\nüîç Test query: 'What is retrieval augmented generation?'\nüìä Found 3 relevant chunks:\n  1. Retrieval-augmented generation (similarity: 0.889)\n     Retrieval-augmented generation (RAG) is a technique that enables large language models (LLMs) to ret...\n  2. Retrieval-augmented generation (similarity: 0.871)\n     Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by incorporating an infor...\n  3. Retrieval-augmented generation (similarity: 0.842)\n     the model‚Äôs external knowledge base with the updated information\" (augmentation). By dynamically int...\n"}],"source":"class SingleStoreRetriever:\n    def __init__(self, connection, embeddings_model):\n        self.connection = connection\n        self.embeddings = embeddings_model\n    \n    def similarity_search(self, query: str, k: int = 5) -> List[Document]:\n        \"\"\"Perform similarity search using cosine similarity\"\"\"\n        \n        # Get query embedding\n        query_embedding = self.embeddings.embed_query(query)\n        query_bytes = np.array(query_embedding, dtype=np.float32).tobytes()\n        \n        # SQL query for cosine similarity\n        search_query = \"\"\"\n        SELECT content, title, url, chunk_index, embedding,\n               DOT_PRODUCT(embedding, %s) / \n               (SQRT(DOT_PRODUCT(embedding, embedding)) * SQRT(DOT_PRODUCT(%s, %s))) as similarity\n        FROM wikipedia_vectors\n        ORDER BY similarity DESC\n        LIMIT %s\n        \"\"\"\n        \n        try:\n            with self.connection.cursor() as cursor:\n                cursor.execute(search_query, (query_bytes, query_bytes, query_bytes, k))\n                results = cursor.fetchall()\n                \n                documents = []\n                for row in results:\n                    doc = Document(\n                        page_content=row[0],\n                        metadata={\n                            'title': row[1],\n                            'url': row[2],\n                            'chunk_index': row[3],\n                            'similarity': float(row[5])\n                        }\n                    )\n                    documents.append(doc)\n                \n                return documents\n        \n        except Exception as e:\n            print(f\"‚ùå Error in similarity search: {e}\")\n            return []\n\n# Create retriever\nretriever = SingleStoreRetriever(conn, processor.embeddings)\n\n# Test retrieval\ntest_query = \"What is retrieval augmented generation?\"\ntest_results = retriever.similarity_search(test_query, k=3)\nprint(f\"\\nüîç Test query: '{test_query}'\")\nprint(f\"üìä Found {len(test_results)} relevant chunks:\")\nfor i, doc in enumerate(test_results):\n    print(f\"  {i+1}. {doc.metadata['title']} (similarity: {doc.metadata['similarity']:.3f})\")\n    print(f\"     {doc.page_content[:100]}...\")"},{"attachments":{},"cell_type":"markdown","id":"48a4d371-83d2-4c18-bc53-f549e01b06c4","metadata":{"language":"python"},"source":"### Step 9: Create LangChain Agent with Tools"},{"cell_type":"code","execution_count":24,"id":"8158dd73-5324-4a9c-b614-42b5eddf37e9","metadata":{"execution":{"iopub.execute_input":"2025-06-05T16:05:19.449166Z","iopub.status.busy":"2025-06-05T16:05:19.448743Z","iopub.status.idle":"2025-06-05T16:05:19.510031Z","shell.execute_reply":"2025-06-05T16:05:19.508985Z","shell.execute_reply.started":"2025-06-05T16:05:19.449141Z"},"language":"python","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"‚úÖ RAG Agent created successfully!\n"},{"name":"stderr","output_type":"stream","text":"/tmp/ipykernel_41/1700688836.py:23: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n  self.memory = ConversationBufferMemory(\n/tmp/ipykernel_41/1700688836.py:87: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n  agent = initialize_agent(\n"}],"source":"from langchain.agents import initialize_agent, Tool, AgentType\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain.schema import BaseRetriever\n\nclass CustomRetriever(BaseRetriever):\n    \"\"\"Custom retriever that works with our SingleStore implementation\"\"\"\n    \n    def __init__(self, singlestore_retriever):\n        self.singlestore_retriever = singlestore_retriever\n    \n    def _get_relevant_documents(self, query: str) -> List[Document]:\n        \"\"\"Retrieve relevant documents\"\"\"\n        return self.singlestore_retriever.similarity_search(query, k=5)\n    \n    async def _aget_relevant_documents(self, query: str) -> List[Document]:\n        \"\"\"Async version of document retrieval\"\"\"\n        return self._get_relevant_documents(query)\n\nclass WikipediaRAGAgent:\n    def __init__(self, retriever, llm):\n        self.retriever = retriever\n        self.llm = llm\n        self.memory = ConversationBufferMemory(\n            memory_key=\"chat_history\", \n            return_messages=True\n        )\n    \n    def search_knowledge_base(self, query: str) -> str:\n        \"\"\"Search the Wikipedia knowledge base\"\"\"\n        docs = self.retriever.similarity_search(query, k=5)\n        \n        if not docs:\n            return \"No relevant information found in the knowledge base.\"\n        \n        # Combine the most relevant chunks\n        context = \"\\n\\n\".join([\n            f\"From '{doc.metadata['title']}':\\n{doc.page_content}\"\n            for doc in docs[:3]  # Use top 3 results\n        ])\n        \n        return context\n    \n    def answer_with_context(self, query: str) -> str:\n        \"\"\"Answer questions using retrieved context\"\"\"\n        # Get relevant context\n        context = self.search_knowledge_base(query)\n        \n        if \"No relevant information found\" in context:\n            return \"I don't have enough information in my knowledge base to answer that question.\"\n        \n        # Create a prompt with context\n        prompt = f\"\"\"\n        Based on the following context from Wikipedia, please answer the question.\n        \n        Context:\n        {context}\n        \n        Question: {query}\n        \n        Answer based on the context provided:\n        \"\"\"\n        \n        try:\n            response = self.llm.invoke(prompt)\n            return response.content if hasattr(response, 'content') else str(response)\n        except Exception as e:\n            return f\"Error generating response: {e}\"\n    \n    def create_agent(self):\n        \"\"\"Create the LangChain agent with tools\"\"\"\n        \n        # Define tools\n        tools = [\n            Tool(\n                name=\"Wikipedia_Knowledge_Base\",\n                func=self.search_knowledge_base,\n                description=\"Search the Wikipedia knowledge base for information about AI, retrieval augmented generation, ML, NLP, Computer Vision, Deep Learning, and database. Use this when you need factual information about these topics.\"\n            ),\n            Tool(\n                name=\"Answer_With_Context\",\n                func=self.answer_with_context,\n                description=\"Answer questions using the Wikipedia knowledge base with proper context. Use this for detailed explanations and comprehensive answers.\"\n            )\n        ]\n        \n        # Create agent\n        agent = initialize_agent(\n            tools=tools,\n            llm=self.llm,\n            agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION,\n            memory=self.memory,\n            verbose=True,\n            handle_parsing_errors=True,\n            max_iterations=3,\n            early_stopping_method=\"generate\"\n        )\n        \n        return agent\n\n# Initialize the LLM and agent\nllm = ChatOpenAI(temperature=0.7, model=\"gpt-3.5-turbo\")\nrag_agent = WikipediaRAGAgent(retriever, llm)\nagent = rag_agent.create_agent()\n\nprint(\"‚úÖ RAG Agent created successfully!\")"},{"attachments":{},"cell_type":"markdown","id":"2fc13ded-3cbd-46f7-8fbe-f1105f930edc","metadata":{"language":"python"},"source":"### Step 10: Interactive Question-Answering System"},{"cell_type":"code","execution_count":25,"id":"875a17f2-fef5-4890-be0d-1bffb6bb745b","metadata":{"execution":{"iopub.execute_input":"2025-06-05T16:07:10.201967Z","iopub.status.busy":"2025-06-05T16:07:10.201600Z","iopub.status.idle":"2025-06-05T16:07:23.984270Z","shell.execute_reply":"2025-06-05T16:07:23.981325Z","shell.execute_reply.started":"2025-06-05T16:07:10.201937Z"},"language":"python","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"ü§ñ Testing the RAG System:\n==================================================\n\n‚ùì Question: What is the difference between machine learning and retrieval augmented generation?\nü§ñ Direct Answer:\nThe main difference between machine learning and retrieval-augmented generation is that retrieval-augmented generation enhances large language models (LLMs) by incorporating an information-retrieval mechanism that allows models to access and utilize additional data beyond their original training set. This means that retrieval-augmented generation not only relies on the training data but also retrieves and incorporates new information from external sources to generate more accurate and contextually relevant responses. In contrast, traditional machine learning models typically rely solely on the data they were trained on and do not have the capability to dynamically integrate new information from external sources.\n--------------------------------------------------\n\n‚ùì Question: How does natural language processing work?\nü§ñ Direct Answer:\nNatural language processing works by providing computers with the ability to process data encoded in natural language. This involves tasks such as speech recognition, text classification, natural language understanding, and natural language generation. Additionally, neural networks have been used for implementing language models in natural language processing since the early 2000s, with advancements such as LSTM helping to improve machine translation and language modeling.\n--------------------------------------------------\n\n‚ùì Question: What are the main applications of a database?\nü§ñ Direct Answer:\nThe main applications of a database include supporting internal operations of organizations, underpinning online interactions with customers and suppliers, holding administrative information, storing specialized data such as engineering data or economic models, managing computerized library systems, flight reservation systems, parts inventory systems, content management systems, and various other applications in areas like accounting, music compositions, movies, banking, manufacturing, and insurance. Additionally, databases provide support for data storage, retrieval, update, transactions, concurrency, data recovery, authorization, access support from remote locations, enforcing constraints, and administering the database effectively through utilities for import, export, monitoring, defragmentation, and analysis. The database engine acts as the core part of the DBMS interacting between the database and the application interface, providing APIs and processor for database languages like SQL to allow applications to interact with and manipulate the database.\n--------------------------------------------------\n\n‚ùì Question: Can you explain what artificial intelligence is?\nü§ñ Direct Answer:\nArtificial intelligence (AI) refers to technology in which machines are designed to mimic human intelligence, such as learning, reasoning, problem-solving, perception, and language understanding. It includes areas such as machine learning, natural language processing, and document AI. AI can be developed in a way that is explainable or interpretable to humans, allowing users to understand the decisions or predictions made by the AI. This is in contrast to the concept of a \"black box\" in machine learning where decisions made by the AI are not easily explained. AI has the potential to impact various domains, but challenges such as overfitting and bias need to be addressed for its effective and fair use.\n--------------------------------------------------\n\n‚ùì Question: What role does retrieval augmented generation play in AI?\nü§ñ Direct Answer:\nRetrieval-augmented generation (RAG) plays a crucial role in enhancing large language models (LLMs) by allowing them to access and utilize additional data beyond their original training set. This technique enables LLMs to retrieve relevant information from external sources, incorporate new information, reduce reliance on static datasets, and generate more accurate and contextually relevant responses. RAG helps AI systems to dynamically integrate relevant data, generate more informed and contextually grounded responses, and adapt to new information without the need for retraining the model. Ultimately, RAG improves the performance and effectiveness of AI systems by expanding their knowledge base and enhancing their ability to generate tailored and engaging answers.\n--------------------------------------------------\n"}],"source":"def ask_agent(question: str):\n    \"\"\"Ask a question to the RAG agent\"\"\"\n    try:\n        response = agent.invoke({\"input\": question})\n        return response.get(\"output\", \"No response generated\")\n    except Exception as e:\n        print(f\"Agent error: {e}\")\n        # Fallback to direct answer\n        return rag_agent.answer_with_context(question)\n\ndef ask_direct_question(question: str):\n    \"\"\"Ask a question directly without agent (simpler approach)\"\"\"\n    return rag_agent.answer_with_context(question)\n\n# Test the agent with various questions\ntest_questions = [\n    \"What is the difference between machine learning and retrieval augmented generation?\",\n    \"How does natural language processing work?\",\n    \"What are the main applications of a database?\",\n    \"Can you explain what artificial intelligence is?\",\n    \"What role does retrieval augmented generation play in AI?\"\n]\n\nprint(\"ü§ñ Testing the RAG System:\")\nprint(\"=\" * 50)\n\nfor question in test_questions:\n    print(f\"\\n‚ùì Question: {question}\")\n    print(\"ü§ñ Direct Answer:\")\n    response = ask_direct_question(question)\n    print(response)\n    print(\"-\" * 50)"},{"cell_type":"code","execution_count":null,"id":"e215e9b0-a5df-4f68-93f8-6d6c63853a76","metadata":{"language":"python","trusted":true},"outputs":[],"source":""}],"metadata":{"jupyterlab":{"notebooks":{"version_major":6,"version_minor":4}},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"},"singlestore_cell_default_language":"python","singlestore_connection":{"connectionID":"","defaultDatabase":""},"singlestore_row_limit":300},"nbformat":4,"nbformat_minor":5}